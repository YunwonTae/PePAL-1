{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These codes are based on this github (https://github.com/roeeaharoni/unsupervised-domain-clusters),\n",
    "# where the paper is called \"Unsupervised Domain Clusters in Pretrained Language Models\". (https://arxiv.org/pdf/2004.02105.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating pusedo-labels in unsupervised manners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook enables to perform the following steps:\n",
    "\n",
    "- Read text files that represent different users\n",
    "\n",
    "- Encode each line in the text files with a pretrained model (e.g., RoBERTa)\n",
    "\n",
    "- Cluster and visualize the encodings (using a GMM)\n",
    "\n",
    "- Save pusedo-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF version 3.0.2\n",
    "# PT version 1.6.0\n",
    "import torch\n",
    "import time\n",
    "from transformers import *\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "MODELS = [\n",
    "        (RobertaModel, RobertaTokenizer, 'roberta-base'),\n",
    "        (RobertaModel, RobertaTokenizer, 'roberta-large'),\n",
    "        (BertModel, BertTokenizer, 'bert-base-uncased'),\n",
    "        (BertModel, BertTokenizer, 'bert-large-cased'),\n",
    "        (BertModel, BertTokenizer, 'bert-base-multilingual-cased'),\n",
    "        (DistilBertModel, DistilBertTokenizer, 'distilbert-base-uncased'),\n",
    "        (OpenAIGPTModel,  OpenAIGPTTokenizer,  'openai-gpt'),\n",
    "        (GPT2Model, GPT2Tokenizer, 'gpt2'),\n",
    "        (CTRLModel,       CTRLTokenizer,       'ctrl'),\n",
    "        (TransfoXLModel,  TransfoXLTokenizer,  'transfo-xl-wt103'),\n",
    "        (XLNetModel, XLNetTokenizer, 'xlnet-base-cased'),\n",
    "        (XLMModel,        XLMTokenizer,        'xlm-mlm-enfr-1024'),\n",
    "        (BertModel, BertTokenizer, 'bert-base-multilingual-cased'),\n",
    "        (XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-base'),\n",
    "        (XLMRobertaModel, XLMRobertaTokenizer, 'xlm-roberta-large')\n",
    "    ]\n",
    "\n",
    "\n",
    "def encode_with_transformers(corpus, models_to_use = ['roberta-large']):\n",
    "    \"\"\"\n",
    "    Encodes the corpus using the models in models_to_use. \n",
    "    Returns a dictionary from a model name to a list of the encoded sentences and their encodings.\n",
    "    The encodings are calculatd by average-pooling the last hidden states for each token. \n",
    "    \"\"\"\n",
    "    model_to_states = {}\n",
    "    for model_class, tokenizer_class, model_name in MODELS:\n",
    "        if model_name not in models_to_use:\n",
    "            continue\n",
    "        print('encoding with {}...'.format(model_name))\n",
    "        model_to_states[model_name] = {}\n",
    "        model_to_states[model_name]['states'] = []\n",
    "        model_to_states[model_name]['sents'] = []\n",
    "\n",
    "        # Load pretrained model/tokenizer\n",
    "        tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "        model = model_class.from_pretrained(model_name)\n",
    "        model.to(torch.device('cuda'))\n",
    "        \n",
    "        # Encode text\n",
    "        start = time.time()\n",
    "        for sentence in corpus:\n",
    "            model_to_states[model_name]['sents'].append(sentence)\n",
    "            input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True, max_length=300)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "            input_ids = input_ids.to(torch.device('cuda'))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                output = model(input_ids)\n",
    "                last_hidden_states = output[0]\n",
    "                \n",
    "                # avg pool last hidden layer\n",
    "                squeezed = last_hidden_states.squeeze(dim=0)\n",
    "                masked = squeezed[:input_ids.shape[1],:]\n",
    "                avg_pooled = masked.mean(dim=0)                \n",
    "                model_to_states[model_name]['states'].append(avg_pooled.cpu())\n",
    "                \n",
    "        end = time.time()\n",
    "        print('encoded with {} in {} seconds'.format(model_name, end - start))\n",
    "        np_tensors = [np.array(tensor) for tensor in model_to_states[model_name]['states']]\n",
    "        model_to_states[model_name]['states'] = np.stack(np_tensors)\n",
    "    return model_to_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot GMM Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(__doc__)\n",
    "hsv = plt.get_cmap('hsv')\n",
    "colors = ['red', 'green', 'blue', 'yellow', 'purple', 'orange', 'pink', 'brown', 'gray', 'black', ]\n",
    "\n",
    "import pdb\n",
    "def fit_gmm(name_to_embeddings, class_names, first_principal_component_shown=0, \n",
    "            last_principal_component_shown=1, clusters=5, header='', plot=True, pca=True, examples_per_class = 388):\n",
    "    \"\"\"\n",
    "    Fits a GMM to the embeddings in name_to_embeddings where each name represents a dataset.\n",
    "    \"\"\"\n",
    "    #pdb.set_trace()\n",
    "    gmm_colors = hsv(np.linspace(0, 1.0, clusters))\n",
    "    colors = hsv(np.linspace(0, 1.0, clusters))\n",
    "    \n",
    "    all_states = []\n",
    "    all_sents = []\n",
    "    num_classes = len(class_names) # number of domains (e.g., users)\n",
    "    \n",
    "    if last_principal_component_shown <= first_principal_component_shown:\n",
    "        raise Exception('first PCA component must be smaller than the 2nd')\n",
    "    \n",
    "    # Concatenate the data to one matrix\n",
    "    for label in class_names:\n",
    "        # all_states.append(name_to_embeddings[label]['states'][0:examples_per_class])\n",
    "        all_states.append(name_to_embeddings[label]['states'][:])\n",
    "        all_sents += name_to_embeddings[label]['sents']\n",
    "    concat_all_embs = np.concatenate(all_states)\n",
    "    \n",
    "    # Compute PCA\n",
    "    if pca:\n",
    "        pca = PCA(n_components=1+last_principal_component_shown)\n",
    "        pca_data = pca.fit_transform(concat_all_embs)[:, list(range(first_principal_component_shown,last_principal_component_shown+1))]\n",
    "    else:\n",
    "        pca_data = concat_all_embs\n",
    "    \n",
    "    #pdb.set_trace()\n",
    "    \n",
    "    pca_labels = []\n",
    "    for i, label in enumerate(class_names): # number of domains\n",
    "        for j in range(len(name_to_embeddings[label]['sents'])): # number of sentences per domain (e.g., # of sentences per users)\n",
    "            pca_labels.append(i)\n",
    "    pca_labels = np.array(pca_labels)\n",
    "    # pdb.set_trace()\n",
    "\n",
    "    # Do not split the data - train=test=all (unsupervised evaluation) \n",
    "    train_index = list(range(0, pca_data.shape[0]))\n",
    "    test_index = list(range(0, pca_data.shape[0]))\n",
    "    \n",
    "    # TODO: why train and test?\n",
    "    X_train = pca_data[train_index]\n",
    "    y_train = pca_labels[train_index]\n",
    "    X_test = pca_data[test_index]\n",
    "    y_test = pca_labels[test_index]\n",
    "\n",
    "    n_classes = len(np.unique(y_train))\n",
    "    if clusters > 0: # number of clusters passed by arguments\n",
    "        n_clusters = clusters\n",
    "    else:\n",
    "        n_clusters = n_classes\n",
    "    \n",
    "    # Can try GMMs using different types of covariances, we use full.\n",
    "    estimators = {cov_type: GaussianMixture(n_components=n_clusters,\n",
    "                  covariance_type=cov_type, max_iter=150, random_state=0)\n",
    "                  for cov_type in ['full']} #'spherical', 'diag', 'tied', \n",
    "\n",
    "    n_estimators = len(estimators)\n",
    "\n",
    "    # Configure the plot\n",
    "    if plot:\n",
    "        main_plot = plt.figure(figsize=(8, 8))\n",
    "        plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05, left=.01, right=.99)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    for index, (name, estimator) in enumerate(estimators.items()):\n",
    "        \n",
    "        # train the GMM         \n",
    "        estimator.fit(X_train)\n",
    "        \n",
    "        # create the plots\n",
    "        if plot == True:\n",
    "            h = plt.subplot(1, 1, 1)\n",
    "\n",
    "            # Plot the train data with dots\n",
    "            for n, color in enumerate(colors[:num_classes]):\n",
    "                temp = []\n",
    "                for index in range(len(pca_labels)):\n",
    "                    if pca_labels[index] == n:\n",
    "                        temp.append(index)\n",
    "                #pdb.set_trace()\n",
    "                data = pca_data[temp]\n",
    "                \n",
    "                plt.scatter(data[:, 0], data[:, 1], s=20, marker='o', color=color, \n",
    "                            label=class_names[n].replace('_dev',''), alpha = 0.3)\n",
    "                \n",
    "                \n",
    "        # predict the cluster ids for train         \n",
    "        y_train_pred = estimator.predict(X_train)\n",
    "        \n",
    "        # predict the cluster ids for test\n",
    "        y_test_pred = estimator.predict(X_test)\n",
    "        \n",
    "        ########################################\n",
    "        # create the plots\n",
    "        if plot == True:\n",
    "            main_plot = plt.figure(figsize=(8, 8))\n",
    "            plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05, left=.01, right=.99)\n",
    "            h = plt.subplot(1, 1, 1)\n",
    "\n",
    "            # Plot the train data with dots\n",
    "            for n, color in enumerate(gmm_colors[:]):\n",
    "                temp = []\n",
    "                for index, label in enumerate(y_train_pred):\n",
    "                    if label == n:\n",
    "                        temp.append(index)\n",
    "                # pdb.set_trace()\n",
    "                data = pca_data[temp]\n",
    "                \n",
    "                plt.scatter(data[:, 0], data[:, 1], s=20, marker='o', color=color, \n",
    "                            alpha = 0.3)\n",
    "        ########################################\n",
    "\n",
    "    if plot:        \n",
    "        plt.suptitle(header)\n",
    "        plt.show()\n",
    "\n",
    "    return y_train_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the each user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#EX)\n",
    "#/home/users/user_109/train.src.tok\n",
    "#/home/users/user_109/train.mt.tok\n",
    "#/home/users/user_109/train.pe.tok\n",
    "#/home/users/user_109/valid.*\n",
    "#/home/users/user_109/test.*\n",
    "\n",
    "###############\n",
    "#CHANGE PATH\n",
    "###############\n",
    "base_path_new = 'PATH_FOR_USERS'\n",
    "\n",
    "users = ['user_109', 'user_129', 'user_150', 'user_159', 'user_29', 'user_50', 'user_76', 'user_95', 'user_120', 'user_130', 'user_151', 'user_160', 'user_34', 'user_56', 'user_81', 'user_97', 'user_121', 'user_144', 'user_152', 'user_18', 'user_43', 'user_59', 'user_84', 'user_122', 'user_148', 'user_156', 'user_19', 'user_44', 'user_64', 'user_94']\n",
    "\n",
    "file_paths_new = {}\n",
    "\n",
    "for user in users:\n",
    "    file_paths_new[user] = base_path_new + '/' + user + \"/train.src.tok\"\n",
    "\n",
    "models_to_use = ['roberta-large']\n",
    "\n",
    "model_to_domain_to_encodings_new = defaultdict(dict)\n",
    "for domain_name in file_paths_new:\n",
    "    print('encoding {} with transformers...'.format(domain_name))\n",
    "    file_path = file_paths_new[domain_name]\n",
    "    counter = Counter(open(file_path).readlines())\n",
    "    print(counter.most_common(10))\n",
    "    lines = open(file_path).readlines()\n",
    "    print('found {} lines'.format(len(lines)))\n",
    "    res = encode_with_transformers(lines, models_to_use)\n",
    "    for model_name in models_to_use:\n",
    "        model_to_domain_to_encodings_new[model_name][domain_name] = res[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = users\n",
    "first_principal = 1\n",
    "last_principal = 50\n",
    "num_clusters = 10\n",
    "num_experiments = 1\n",
    "use_pca = True\n",
    "\n",
    "model_to_accuracies = defaultdict(list)\n",
    "for i in range(num_experiments):\n",
    "    if i == num_experiments - 1:\n",
    "        plot = True\n",
    "    else:\n",
    "        plot = False\n",
    "    \n",
    "    for model_name in model_to_domain_to_encodings_new:\n",
    "        y_train_pred = fit_gmm(model_to_domain_to_encodings_new[model_name], domains, \n",
    "                                first_principal_component_shown = first_principal,\n",
    "                                last_principal_component_shown = last_principal, \n",
    "                                clusters = num_clusters,\n",
    "                                header = model_name, plot=plot, pca = use_pca, examples_per_class=388)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save pusedo-labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_by_domains = {}\n",
    "accumulate_sent = 0\n",
    "num_sent = 0\n",
    "for domain_name in file_paths_new:\n",
    "    file_path = file_paths_new[domain_name]\n",
    "    lines = open(file_path).readlines()\n",
    "    accumulate_sent += num_sent\n",
    "    num_sent = len(lines)\n",
    "    with open(base_path_new+'/'+domain_name+'/train_domain.10', 'w') as f:\n",
    "        for label in y_train_pred[accumulate_sent:accumulate_sent+num_sent]:\n",
    "            f.write(str(label)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, domain_name in enumerate(file_paths_new):\n",
    "    lines = open(base_path_new+'/'+domain_name+'/train.src.tok').readlines()  \n",
    "    accumulate_sent += num_sent\n",
    "    num_sent = len(lines)\n",
    "    with open(base_path_new+'/'+domain_name+'/train.USER', 'w') as f:\n",
    "        for label in range(num_sent):\n",
    "            f.write(str(user)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, domain_name in enumerate(file_paths_new):\n",
    "    lines = open(base_path_new+'/'+domain_name+'/valid.src.tok').readlines()   \n",
    "    accumulate_sent += num_sent  \n",
    "    num_sent = len(lines)\n",
    "    with open(base_path_new+'/'+domain_name+'/valid.USER', 'w') as f:\n",
    "        for label in range(num_sent):\n",
    "            f.write(str(user)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user, domain_name in enumerate(file_paths_new):\n",
    "    lines = open(base_path_new+'/'+domain_name+'/test.src.tok').readlines()  \n",
    "    accumulate_sent += num_sent\n",
    "    num_sent = len(lines)\n",
    "    with open(base_path_new+'/'+domain_name+'/test.USER', 'w') as f:\n",
    "        for label in range(num_sent):\n",
    "            f.write(str(user)+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data shuffle and save for train, valid, and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = []\n",
    "for user, domain_name in enumerate(file_paths_new):\n",
    "    temp = {}\n",
    "    for type_ in ['src','mt','pe']:\n",
    "        lines = open(base_path_new+'/'+domain_name+'/train.' + type_ + '.tok').readlines()\n",
    "        temp[type_] = lines\n",
    "    lines = open(base_path_new+'/'+domain_name+'/train_domain.10').readlines()\n",
    "    temp['domain_15'] = lines\n",
    "    lines = open(base_path_new+'/'+domain_name+'/train.USER').readlines()\n",
    "    temp['USER'] = lines\n",
    "    train_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_df = []\n",
    "for user, domain_name in enumerate(file_paths_new):\n",
    "    temp = {}\n",
    "    for type_ in ['src','mt','pe']:\n",
    "        lines = open(base_path_new+'/'+domain_name+'/valid.' + type_ + '.tok').readlines()\n",
    "        temp[type_] = lines\n",
    "    lines = open(base_path_new+'/'+domain_name+'/valid.USER').readlines()\n",
    "    temp['USER'] = lines\n",
    "    valid_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = []\n",
    "for user, domain_name in enumerate(file_paths_new):\n",
    "    temp = {}\n",
    "    for type_ in ['src','mt','pe']:\n",
    "        lines = open(base_path_new+'/'+domain_name+'/test.' + type_ + '.tok').readlines()\n",
    "        temp[type_] = lines\n",
    "    lines = open(base_path_new+'/'+domain_name+'/test.USER').readlines()\n",
    "    temp['USER'] = lines\n",
    "    test_df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = [user['src'] for user in train_df]\n",
    "train_mt = [user['mt'] for user in train_df]\n",
    "train_pe = [user['pe'] for user in train_df]\n",
    "train_10 = [user['domain_10'] for user in train_df]\n",
    "train_user = [user['USER'] for user in train_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_src = [user['src'] for user in valid_df]\n",
    "valid_mt = [user['mt'] for user in valid_df]\n",
    "valid_pe = [user['pe'] for user in valid_df]\n",
    "valid_user = [user['USER'] for user in valid_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_src = [user['src'] for user in test_df]\n",
    "test_mt = [user['mt'] for user in test_df]\n",
    "test_pe = [user['pe'] for user in test_df]\n",
    "test_user = [user['USER'] for user in test_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data):\n",
    "    #dataframe\n",
    "    a = []\n",
    "    for sublist in data:\n",
    "        a.append(pd.DataFrame(sublist))\n",
    "\n",
    "    b = pd.DataFrame()\n",
    "    for item in a:\n",
    "         b= pd.concat([b, item])\n",
    "    return b.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_shuffle_and_clean(src,mt,pe,_10,USER):\n",
    "    shuffle_ = pd.concat([src, mt, pe, _10, USER], axis=1)\n",
    "    shuffle_ = shuffle(shuffle_)\n",
    "    shuffle_.columns=['src','mt','pe', '15', 'user']\n",
    "    shuffle_1 = shuffle_[['src']]\n",
    "    shuffle_2 = shuffle_[['mt']]\n",
    "    shuffle_3 = shuffle_[['pe']]\n",
    "    shuffle_4 = shuffle_[['10']]\n",
    "    shuffle_5 = shuffle_[['user']]\n",
    "    return shuffle_1, shuffle_2, shuffle_3, shuffle_4, shuffle_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_clean(src,mt,pe,USER):\n",
    "    shuffle_ = pd.concat([src, mt, pe, USER], axis=1)\n",
    "    shuffle_ = shuffle(shuffle_)\n",
    "    shuffle_.columns=['src','mt','pe','user']\n",
    "    shuffle_1 = shuffle_[['src']]\n",
    "    shuffle_2 = shuffle_[['mt']]\n",
    "    shuffle_3 = shuffle_[['pe']]\n",
    "    shuffle_4 = shuffle_[['user']]\n",
    "    return shuffle_1, shuffle_2, shuffle_3, shuffle_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = create_dataframe(train_src)\n",
    "train_mt = create_dataframe(train_mt)\n",
    "train_pe = create_dataframe(train_pe)\n",
    "train_10 = create_dataframe(train_10)\n",
    "train_user = create_dataframe(train_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_src = create_dataframe(valid_src)\n",
    "valid_mt = create_dataframe(valid_mt)\n",
    "valid_pe = create_dataframe(valid_pe)\n",
    "valid_user = create_dataframe(valid_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_src = create_dataframe(test_src)\n",
    "test_mt = create_dataframe(test_mt)\n",
    "test_pe = create_dataframe(test_pe)\n",
    "test_user = create_dataframe(test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_train_src, shuffle_train_mt, shuffle_train_pe, shuffle_train_10, shuffle_train_user = train_shuffle_and_clean(train_src, train_mt, train_pe, train_10, train_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_valid_src, shuffle_valid_mt, shuffle_valid_pe, shuffle_valid_user = shuffle_and_clean(valid_src, valid_mt, valid_pe, valid_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle_test_src, shuffle_test_mt, shuffle_test_pe, shuffle_test_user = shuffle_and_clean(test_src, test_mt, test_pe, test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(data, path):\n",
    "    with open(path, 'w') as f:\n",
    "        for user in data.values:\n",
    "            for sample in user:\n",
    "                sample = unicodedata.normalize('NFC',sample).encode().decode()\n",
    "                f.write(\"%s\" % sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'WHERE_TO_SAVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(shuffle_train_src, save_path + 'train.src')\n",
    "save_data(shuffle_train_mt, save_path + 'train.mt.tok')\n",
    "save_data(shuffle_train_pe, save_path + 'train.pe.tok')\n",
    "save_data(shuffle_train_10, save_path + 'train.10')\n",
    "save_data(shuffle_train_user, save_path + 'train.USER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(shuffle_valid_src, save_path + 'valid.src')\n",
    "save_data(shuffle_valid_mt, save_path + 'valid.mt.tok')\n",
    "save_data(shuffle_valid_pe, save_path + 'valid.pe.tok')\n",
    "######### \n",
    "#Altough we don't use pusedo-labels during validation and test phase, \n",
    "#we need these for dummy data so that we can simplify the codes.\n",
    "save_data(shuffle_valid_user, save_path + 'valid.10')\n",
    "#########\n",
    "save_data(shuffle_valid_user, save_path + 'valid.USER')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(shuffle_test_src, save_path + 'test.src')\n",
    "save_data(shuffle_test_mt, save_path + 'test.mt.tok')\n",
    "save_data(shuffle_test_pe, save_path + 'test.pe.tok')\n",
    "######### \n",
    "#Altough we don't use pusedo-labels during validation and test phase, \n",
    "#we need these for dummy data so that we can simplify the codes.\n",
    "save_data(shuffle_valid_user, save_path + 'test.10')\n",
    "#########\n",
    "save_data(shuffle_test_user, save_path + 'test.USER')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
